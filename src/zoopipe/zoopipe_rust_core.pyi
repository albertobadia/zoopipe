from typing import Any, Dict, List, Optional

def get_version() -> str: ...
def get_file_size(path: str) -> int: ...
def get_iceberg_data_files(table_location: str) -> List[str]: ...
def commit_iceberg_transaction(
    table_location: str,
    catalog_properties: Dict[str, str],
    data_files_json: List[str],
) -> None: ...

class DeltaTransactionHandle:
    """Opaque handle for Delta Lake actions."""

    pass

def get_delta_files(
    table_uri: str,
    version: Optional[int] = None,
    storage_options: Optional[Dict[str, str]] = None,
) -> List[str]: ...
def commit_delta_transaction(
    table_uri: str,
    handles: List[DeltaTransactionHandle],
    mode: str,
    storage_options: Optional[Dict[str, str]] = None,
) -> None: ...

class DeltaReader:
    def __init__(
        self,
        table_uri: str,
        version: Optional[int] = None,
        storage_options: Optional[Dict[str, str]] = None,
        batch_size: int = 1024,
        files: Optional[List[str]] = None,
    ) -> None: ...
    def read_batch(self, batch_size: int) -> Optional[List[Any]]: ...

class DeltaWriter:
    def __init__(
        self,
        table_uri: str,
        storage_options: Optional[Dict[str, str]] = None,
    ) -> None: ...
    def write_batch(self, entries: Any) -> None: ...
    def close(self) -> Optional[DeltaTransactionHandle]: ...

class SingleThreadExecutor:
    def __init__(self, batch_size: int = 1000) -> None: ...
    def get_batch_size(self) -> int: ...
    def get_concurrency(self) -> int: ...

class MultiThreadExecutor:
    def __init__(
        self, max_workers: Optional[int] = None, batch_size: int = 1000
    ) -> None: ...
    def get_batch_size(self) -> int: ...
    def get_concurrency(self) -> int: ...

class NativePipe:
    def __init__(
        self,
        reader: Any,
        writer: Any,
        error_writer: Optional[Any],
        batch_processor: Any,
        report: Any,
        report_update_interval: int,
        executor: Any,
    ) -> None: ...
    def run(self) -> Optional[str]: ...

class CSVReader:
    def __init__(
        self,
        path: str,
        delimiter: int = 44,
        quote: int = 34,
        skip_rows: int = 0,
        fieldnames: Optional[List[str]] = None,
        generate_ids: bool = True,
        limit: Optional[int] = None,
        start_byte: int = 0,
        end_byte: Optional[int] = None,
    ) -> None: ...
    @staticmethod
    def from_bytes(
        data: bytes,
        delimiter: int = 44,
        quote: int = 34,
        skip_rows: int = 0,
        fieldnames: Optional[List[str]] = None,
        generate_ids: bool = True,
        limit: Optional[int] = None,
    ) -> "CSVReader": ...
    @staticmethod
    def count_rows(
        path: str, delimiter: int = 44, quote: int = 34, has_header: bool = True
    ) -> int: ...
    @property
    def headers(self) -> List[str]: ...
    def read_batch(self, batch_size: int) -> Optional[List[Any]]: ...

class JSONReader:
    def __init__(
        self,
        path: str,
        limit: Optional[int] = None,
        generate_ids: bool = True,
        start_byte: int = 0,
        end_byte: Optional[int] = None,
    ) -> None: ...
    def read_batch(self, batch_size: int) -> Optional[List[Any]]: ...

class ArrowReader:
    def __init__(self, path: str, generate_ids: bool = True) -> None: ...
    def read_batch(self, batch_size: int) -> Optional[List[Any]]: ...

class ParquetReader:
    def __init__(
        self, path: str, generate_ids: bool = True, batch_size: int = 1024
    ) -> None: ...
    def read_batch(self, batch_size: int) -> Optional[List[Any]]: ...

class MultiParquetReader:
    def __init__(
        self, paths: List[str], generate_ids: bool = True, batch_size: int = 1024
    ) -> None: ...
    def read_batch(self, batch_size: int) -> Optional[List[Any]]: ...

class ExcelReader:
    def __init__(
        self,
        path: str,
        sheet_name: Optional[str] = None,
        header_row: Optional[int] = 0,
        generate_ids: bool = True,
    ) -> None: ...
    def read_batch(self, batch_size: int) -> Optional[List[Any]]: ...

class SQLReader:
    def __init__(
        self,
        connection_string: str,
        query: str,
        params: Optional[Dict[str, Any]] = None,
        generate_ids: bool = True,
        batch_size: int = 1000,
    ) -> None: ...
    def read_batch(self, batch_size: int) -> Optional[List[Any]]: ...

class KafkaReader:
    def __init__(
        self, uri: str, group_id: Optional[str] = None, generate_ids: bool = True
    ) -> None: ...
    def read_batch(self, batch_size: int) -> Optional[List[Any]]: ...

class PyGeneratorReader:
    def __init__(self, iterable: Any, generate_ids: bool = True) -> None: ...
    def read_batch(self, batch_size: int) -> Optional[List[Any]]: ...

class CSVWriter:
    def __init__(
        self,
        path: str,
        delimiter: int = 44,
        quote: int = 34,
        fieldnames: Optional[List[str]] = None,
    ) -> None: ...
    def write_batch(self, entries: List[Any]) -> None: ...
    def close(self) -> None: ...

class JSONWriter:
    def __init__(self, path: str, format: str = "jsonl") -> None: ...
    def write_batch(self, entries: List[Any]) -> None: ...
    def close(self) -> None: ...

class ParquetWriter:
    def __init__(
        self,
        path: str,
        schema: Optional[Any] = None,
        compression: Optional[str] = "snappy",
    ) -> None: ...
    def write_batch(self, entries: List[Any]) -> None: ...
    def close(self) -> str: ...

class ArrowWriter:
    def __init__(self, path: str, schema: Optional[Any] = None) -> None: ...
    def write_batch(self, entries: List[Any]) -> None: ...
    def close(self) -> str: ...

class ExcelWriter:
    def __init__(self, path: str, sheet_name: str = "Sheet1") -> None: ...
    def write_batch(self, entries: List[Any]) -> None: ...
    def close(self) -> None: ...

class SQLWriter:
    def __init__(
        self,
        connection_string: str,
        table: str,
        batch_size: int = 1000,
        method: str = "append",
    ) -> None: ...
    def write_batch(self, entries: List[Any]) -> None: ...
    def close(self) -> None: ...

class KafkaWriter:
    def __init__(self, uri: str, acks: int = 1, timeout: int = 30) -> None: ...
    def write_batch(self, entries: List[Any]) -> None: ...
    def close(self) -> None: ...

class IcebergWriter:
    def __init__(self, table_location: str) -> None: ...
    def write_batch(self, entries: List[Any]) -> None: ...
    def close(self) -> str: ...

class PyGeneratorWriter:
    def __init__(self, queue_size: int = 1000) -> None: ...
    def write_batch(self, entries: List[Any]) -> None: ...
    def close(self) -> None: ...
